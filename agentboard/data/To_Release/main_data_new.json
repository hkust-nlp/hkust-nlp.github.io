[
    {
        "model": "GPT-4",
        "tasks": {
            "AlfWorld": {
                "score": "0.655",
                "accuracy": "0.433",
                "grounding": "0.826"
            },
            "ScienceWorld": {
                "score": "0.788",
                "accuracy": "0.522",
                "grounding": "0.826"
            },
            "BabyAI": {
                "score": "0.707",
                "accuracy": "0.563",
                "grounding": "0.826"
            },
            "Embodied": {
                "score": "0.717",
                "accuracy": "0.506",
                "grounding": "0.826"
            },
            "Jericho": {
                "score": "0.524",
                "accuracy": "0.350",
                "grounding": "1.0"
            },
            "PDDL": {
                "score": "0.812",
                "accuracy": "0.617",
                "grounding": "0.93"
            },
            "Game": {
                "score": "0.668",
                "accuracy": "0.484",
                "grounding": "0.965"
            },
            "WebShop": {
                "score": "0.769",
                "accuracy": "0.390",
                "grounding": "0.983"
            },
            "WebArena": {
                "score": "0.394",
                "accuracy": "0.151",
                "grounding": "0.976"
            },
            "web": {
                "score": "0.581",
                "accuracy": "0.271",
                "grounding": "0.979"
            },
            "Tool-Query": {
                "score": "0.852",
                "accuracy": "0.683",
                "grounding": "0.975"
            },
            "Tool-Operation": {
                "score": "0.808",
                "accuracy": "0.600",
                "grounding": "0.985"
            },
            "tools": {
                "score": "0.830",
                "accuracy": "0.642",
                "grounding": "0.98"
            },
            "Avg": {
                "score": "0.701",
                "accuracy": "0.479",
                "grounding": "0.925"
            }
        }
    },
    {
        "model": "Claude2",
        "tasks": {
            "AlfWorld": {
                "score": "0.341",
                "accuracy": "0.246",
                "grounding": "0.574"
            },
            "ScienceWorld": {
                "score": "0.320",
                "accuracy": "0.111",
                "grounding": "0.112"
            },
            "BabyAI": {
                "score": "0.375",
                "accuracy": "0.481",
                "grounding": "0.616"
            },
            "Embodied": {
                "score": "0.345",
                "accuracy": "0.279",
                "grounding": "0.434"
            },
            "Jericho": {
                "score": "0.204",
                "accuracy": "0.000",
                "grounding": "0.982"
            },
            "PDDL": {
                "score": "0.614",
                "accuracy": "0.400",
                "grounding": "0.712"
            },
            "Game": {
                "score": "0.409",
                "accuracy": "0.200",
                "grounding": "0.847"
            },
            "WebShop": {
                "score": "0.746",
                "accuracy": "0.378",
                "grounding": "0.959"
            },
            "WebArena": {
                "score": "0.364",
                "accuracy": "0.086",
                "grounding": "0.839"
            },
            "web": {
                "score": "0.555",
                "accuracy": "0.232",
                "grounding": "0.899"
            },
            "Tool-Query": {
                "score": "0.735",
                "accuracy": "0.483",
                "grounding": "0.937"
            },
            "Tool-Operation": {
                "score": "0.614",
                "accuracy": "0.300",
                "grounding": "0.907"
            },
            "tools": {
                "score": "0.675",
                "accuracy": "0.392",
                "grounding": "0.922"
            },
            "Avg": {
                "score": "0.479",
                "accuracy": "0.276",
                "grounding": "0.738"
            }
        }
    },
    {
        "model": "GPT-3.5-Turbo",
        "tasks": {
            "AlfWorld": {
                "score": "0.356",
                "accuracy": "0.172",
                "grounding": "0.592"
            },
            "ScienceWorld": {
                "score": "0.319",
                "accuracy": "0.118",
                "grounding": "0.187"
            },
            "BabyAI": {
                "score": "0.517",
                "accuracy": "0.393",
                "grounding": "0.624"
            },
            "Embodied": {
                "score": "0.397",
                "accuracy": "0.228",
                "grounding": "0.468"
            },
            "Jericho": {
                "score": "0.199",
                "accuracy": "0.050",
                "grounding": "0.998"
            },
            "PDDL": {
                "score": "0.250",
                "accuracy": "0.050",
                "grounding": "0.66"
            },
            "Game": {
                "score": "0.225",
                "accuracy": "0.050",
                "grounding": "0.829"
            },
            "WebShop": {
                "score": "0.764",
                "accuracy": "0.351",
                "grounding": "0.902"
            },
            "WebArena": {
                "score": "0.255",
                "accuracy": "0.046",
                "grounding": "0.913"
            },
            "web": {
                "score": "0.510",
                "accuracy": "0.198",
                "grounding": "0.907"
            },
            "Tool-Query": {
                "score": "0.695",
                "accuracy": "0.450",
                "grounding": "0.976"
            },
            "Tool-Operation": {
                "score": "0.372",
                "accuracy": "0.075",
                "grounding": "0.918"
            },
            "tools": {
                "score": "0.534",
                "accuracy": "0.263",
                "grounding": "0.947"
            },
            "Avg": {
                "score": "0.414",
                "accuracy": "0.189",
                "grounding": "0.752"
            }
        }
    },
    {
        "model": "GPT-3.5-Turbo-16k",
        "tasks": {
            "AlfWorld": {
                "score": "0.252",
                "accuracy": "0.045",
                "grounding": "0.574"
            },
            "ScienceWorld": {
                "score": "0.022",
                "accuracy": "0.000",
                "grounding": "0.092"
            },
            "BabyAI": {
                "score": "0.451",
                "accuracy": "0.339",
                "grounding": "0.733"
            },
            "Embodied": {
                "score": "0.242",
                "accuracy": "0.128",
                "grounding": "0.466"
            },
            "Jericho": {
                "score": "0.161",
                "accuracy": "0.000",
                "grounding": "1.0"
            },
            "PDDL": {
                "score": "0.226",
                "accuracy": "0.033",
                "grounding": "0.776"
            },
            "Game": {
                "score": "0.194",
                "accuracy": "0.017",
                "grounding": "0.888"
            },
            "WebShop": {
                "score": "0.738",
                "accuracy": "0.279",
                "grounding": "0.966"
            },
            "WebArena": {
                "score": "0.237",
                "accuracy": "0.061",
                "grounding": "0.813"
            },
            "web": {
                "score": "0.487",
                "accuracy": "0.170",
                "grounding": "0.89"
            },
            "Tool-Query": {
                "score": "0.591",
                "accuracy": "0.317",
                "grounding": "0.979"
            },
            "Tool-Operation": {
                "score": "0.392",
                "accuracy": "0.150",
                "grounding": "0.922"
            },
            "tools": {
                "score": "0.492",
                "accuracy": "0.234",
                "grounding": "0.951"
            },
            "Avg": {
                "score": "0.341",
                "accuracy": "0.136",
                "grounding": "0.762"
            }
        }
    },
    {
        "model": "Text-Davinci-003",
        "tasks": {
            "AlfWorld": {
                "score": "0.188",
                "accuracy": "0.090",
                "grounding": "0.273"
            },
            "ScienceWorld": {
                "score": "0.289",
                "accuracy": "0.078",
                "grounding": "0.172"
            },
            "BabyAI": {
                "score": "0.175",
                "accuracy": "0.143",
                "grounding": "0.159"
            },
            "Embodied": {
                "score": "0.217",
                "accuracy": "0.104",
                "grounding": "0.201"
            },
            "Jericho": {
                "score": "0.286",
                "accuracy": "0.100",
                "grounding": "0.979"
            },
            "PDDL": {
                "score": "0.317",
                "accuracy": "0.117",
                "grounding": "0.726"
            },
            "Game": {
                "score": "0.302",
                "accuracy": "0.109",
                "grounding": "0.853"
            },
            "WebShop": {
                "score": "0.723",
                "accuracy": "0.295",
                "grounding": "0.974"
            },
            "WebArena": {
                "score": "0.162",
                "accuracy": "0.025",
                "grounding": "0.237"
            },
            "web": {
                "score": "0.442",
                "accuracy": "0.160",
                "grounding": "0.605"
            },
            "Tool-Query": {
                "score": "0.650",
                "accuracy": "0.383",
                "grounding": "0.952"
            },
            "Tool-Operation": {
                "score": "0.562",
                "accuracy": "0.225",
                "grounding": "0.825"
            },
            "tools": {
                "score": "0.606",
                "accuracy": "0.304",
                "grounding": "0.889"
            },
            "Avg": {
                "score": "0.372",
                "accuracy": "0.162",
                "grounding": "0.857"
            }
        }
    },
    {
        "model": "Llama2-13b",
        "tasks": {
            "AlfWorld": {
                "score": "0.071",
                "accuracy": "0.000",
                "grounding": "0.102"
            },
            "ScienceWorld": {
                "score": "0.017",
                "accuracy": "0.000",
                "grounding": "0.041"
            },
            "BabyAI": {
                "score": "0.300",
                "accuracy": "0.134",
                "grounding": "0.571"
            },
            "Embodied": {
                "score": "0.129",
                "accuracy": "0.045",
                "grounding": "0.238"
            },
            "Jericho": {
                "score": "0.090",
                "accuracy": "0.000",
                "grounding": "0.963"
            },
            "PDDL": {
                "score": "0.017",
                "accuracy": "0.000",
                "grounding": "0.319"
            },
            "Game": {
                "score": "0.053",
                "accuracy": "0.000",
                "grounding": "0.641"
            },
            "WebShop": {
                "score": "0.595",
                "accuracy": "0.096",
                "grounding": "0.682"
            },
            "WebArena": {
                "score": "0.079",
                "accuracy": "0.020",
                "grounding": "0.372"
            },
            "web": {
                "score": "0.356",
                "accuracy": "0.064",
                "grounding": "0.527"
            },
            "Tool-Query": {
                "score": "0.368",
                "accuracy": "0.000",
                "grounding": "0.87"
            },
            "Tool-Operation": {
                "score": "0.293",
                "accuracy": "0.000",
                "grounding": "0.741"
            },
            "tools": {
                "score": "0.331",
                "accuracy": "0.000",
                "grounding": "0.805"
            },
            "Avg": {
                "score": "0.197",
                "accuracy": "0.034",
                "grounding": "0.518"
            }
        }
    },
    {
        "model": "Llama2-70b",
        "tasks": {
            "AlfWorld": {
                "score": "0.132",
                "accuracy": "0.030",
                "grounding": "0.208"
            },
            "ScienceWorld": {
                "score": "0.026",
                "accuracy": "0.000",
                "grounding": "0.03"
            },
            "BabyAI": {
                "score": "0.300",
                "accuracy": "0.196",
                "grounding": "0.423"
            },
            "Embodied": {
                "score": "0.153",
                "accuracy": "0.075",
                "grounding": "0.22"
            },
            "Jericho": {
                "score": "0.078",
                "accuracy": "0.000",
                "grounding": "0.967"
            },
            "PDDL": {
                "score": "0.081",
                "accuracy": "0.017",
                "grounding": "0.306"
            },
            "Game": {
                "score": "0.080",
                "accuracy": "0.008",
                "grounding": "0.636"
            },
            "WebShop": {
                "score": "0.536",
                "accuracy": "0.131",
                "grounding": "0.593"
            },
            "WebArena": {
                "score": "0.116",
                "accuracy": "0.033",
                "grounding": "0.936"
            },
            "web": {
                "score": "0.356",
                "accuracy": "0.084",
                "grounding": "0.765"
            },
            "Tool-Query": {
                "score": "0.483",
                "accuracy": "0.000",
                "grounding": "0.961"
            },
            "Tool-Operation": {
                "score": "0.380",
                "accuracy": "0.000",
                "grounding": "0.804"
            },
            "tools": {
                "score": "0.432",
                "accuracy": "0.000",
                "grounding": "0.883"
            },
            "Avg": {
                "score": "0.237",
                "accuracy": "0.045",
                "grounding": "0.581"
            }
        }
    },
    {
        "model": "CodeLlama-13b",
        "tasks": {
            "AlfWorld": {
                "score": "0.134",
                "accuracy": "0.022",
                "grounding": "0.158"
            },
            "ScienceWorld": {
                "score": "0.096",
                "accuracy": "0.022",
                "grounding": "0.086"
            },
            "BabyAI": {
                "score": "0.170",
                "accuracy": "0.222",
                "grounding": "0.346"
            },
            "Embodied": {
                "score": "0.133",
                "accuracy": "0.089",
                "grounding": "0.197"
            },
            "Jericho": {
                "score": "0.000",
                "accuracy": "0.000",
                "grounding": "0.997"
            },
            "PDDL": {
                "score": "0.093",
                "accuracy": "0.017",
                "grounding": "0.178"
            },
            "Game": {
                "score": "0.047",
                "accuracy": "0.009",
                "grounding": "0.588"
            },
            "WebShop": {
                "score": "0.655",
                "accuracy": "0.259",
                "grounding": "0.78"
            },
            "WebArena": {
                "score": "0.177",
                "accuracy": "0.037",
                "grounding": "0.821"
            },
            "web": {
                "score": "0.416",
                "accuracy": "0.148",
                "grounding": "0.801"
            },
            "Tool-Query": {
                "score": "0.466",
                "accuracy": "0.250",
                "grounding": "0.905"
            },
            "Tool-Operation": {
                "score": "0.418",
                "accuracy": "0.125",
                "grounding": "0.792"
            },
            "tools": {
                "score": "0.442",
                "accuracy": "0.188",
                "grounding": "0.849"
            },
            "Avg": {
                "score": "0.245",
                "accuracy": "0.106",
                "grounding": "0.563"
            }
        }
    },
    {
        "model": "CodeLlama-34b",
        "tasks": {
            "AlfWorld": {
                "score": "0.113",
                "accuracy": "0.030",
                "grounding": "0.084"
            },
            "ScienceWorld": {
                "score": "0.035",
                "accuracy": "0.000",
                "grounding": "0.09"
            },
            "BabyAI": {
                "score": "0.199",
                "accuracy": "0.134",
                "grounding": "0.28"
            },
            "Embodied": {
                "score": "0.116",
                "accuracy": "0.055",
                "grounding": "0.151"
            },
            "Jericho": {
                "score": "0.155",
                "accuracy": "0.000",
                "grounding": "0.973"
            },
            "PDDL": {
                "score": "0.185",
                "accuracy": "0.033",
                "grounding": "0.436"
            },
            "Game": {
                "score": "0.170",
                "accuracy": "0.017",
                "grounding": "0.705"
            },
            "WebShop": {
                "score": "0.717",
                "accuracy": "0.235",
                "grounding": "0.983"
            },
            "WebArena": {
                "score": "0.212",
                "accuracy": "0.041",
                "grounding": "0.968"
            },
            "web": {
                "score": "0.465",
                "accuracy": "0.138",
                "grounding": "0.975"
            },
            "Tool-Query": {
                "score": "0.600",
                "accuracy": "0.133",
                "grounding": "0.975"
            },
            "Tool-Operation": {
                "score": "0.488",
                "accuracy": "0.075",
                "grounding": "0.828"
            },
            "tools": {
                "score": "0.544",
                "accuracy": "0.104",
                "grounding": "0.901"
            },
            "Avg": {
                "score": "0.300",
                "accuracy": "0.076",
                "grounding": "0.624"
            }
        }
    },
    {
        "model": "Vicuna-13b-16k",
        "tasks": {
            "AlfWorld": {
                "score": "0.110",
                "accuracy": "0.014",
                "grounding": "0.172"
            },
            "ScienceWorld": {
                "score": "0.141",
                "accuracy": "0.022",
                "grounding": "0.241"
            },
            "BabyAI": {
                "score": "0.143",
                "accuracy": "0.054",
                "grounding": "0.745"
            },
            "Embodied": {
                "score": "0.131",
                "accuracy": "0.030",
                "grounding": "0.386"
            },
            "Jericho": {
                "score": "0.152",
                "accuracy": "0.000",
                "grounding": "1.0"
            },
            "PDDL": {
                "score": "0.073",
                "accuracy": "0.017",
                "grounding": "0.592"
            },
            "Game": {
                "score": "0.113",
                "accuracy": "0.009",
                "grounding": "0.796"
            },
            "WebShop": {
                "score": "0.733",
                "accuracy": "0.219",
                "grounding": "0.941"
            },
            "WebArena": {
                "score": "0.113",
                "accuracy": "0.029",
                "grounding": "0.587"
            },
            "web": {
                "score": "0.423",
                "accuracy": "0.124",
                "grounding": "0.764"
            },
            "Tool-Query": {
                "score": "0.343",
                "accuracy": "0.033",
                "grounding": "0.979"
            },
            "Tool-Operation": {
                "score": "0.269",
                "accuracy": "0.000",
                "grounding": "0.923"
            },
            "tools": {
                "score": "0.306",
                "accuracy": "0.017",
                "grounding": "0.951"
            },
            "Avg": {
                "score": "0.231",
                "accuracy": "0.043",
                "grounding": "0.687"
            }
        }
    },
    {
        "model": "Lemur-70b",
        "tasks": {
            "AlfWorld": {
                "score": "0.108",
                "accuracy": "0.007",
                "grounding": "0.157"
            },
            "ScienceWorld": {
                "score": "0.340",
                "accuracy": "0.056",
                "grounding": "0.478"
            },
            "BabyAI": {
                "score": "0.194",
                "accuracy": "0.098",
                "grounding": "0.446"
            },
            "Embodied": {
                "score": "0.214",
                "accuracy": "0.054",
                "grounding": "0.36"
            },
            "Jericho": {
                "score": "0.101",
                "accuracy": "0.000",
                "grounding": "0.977"
            },
            "PDDL": {
                "score": "0.097",
                "accuracy": "0.033",
                "grounding": "0.31"
            },
            "Game": {
                "score": "0.099",
                "accuracy": "0.017",
                "grounding": "0.643"
            },
            "WebShop": {
                "score": "0.718",
                "accuracy": "0.116",
                "grounding": "0.84"
            },
            "WebArena": {
                "score": "0.058",
                "accuracy": "0.000",
                "grounding": "0.191"
            },
            "web": {
                "score": "0.388",
                "accuracy": "0.058",
                "grounding": "0.515"
            },
            "Tool-Query": {
                "score": "0.720",
                "accuracy": "0.283",
                "grounding": "0.965"
            },
            "Tool-Operation": {
                "score": "0.359",
                "accuracy": "0.075",
                "grounding": "0.9"
            },
            "tools": {
                "score": "0.540",
                "accuracy": "0.179",
                "grounding": "0.933"
            },
            "Avg": {
                "score": "0.299",
                "accuracy": "0.074",
                "grounding": "0.585"
            }
        }
    },
    {
        "model": "DeepSeek-67b",
        "tasks": {
            "AlfWorld": {
                "score": "0.345",
                "accuracy": "0.209",
                "grounding": "0.436"
            },
            "ScienceWorld": {
                "score": "0.361",
                "accuracy": "0.100",
                "grounding": "0.126"
            },
            "BabyAI": {
                "score": "0.317",
                "accuracy": "0.223",
                "grounding": "0.654"
            },
            "Embodied": {
                "score": "0.341",
                "accuracy": "0.177",
                "grounding": "0.405"
            },
            "Jericho": {
                "score": "0.137",
                "accuracy": "0.000",
                "grounding": "0.998"
            },
            "PDDL": {
                "score": "0.220",
                "accuracy": "0.067",
                "grounding": "0.627"
            },
            "Game": {
                "score": "0.178",
                "accuracy": "0.033",
                "grounding": "0.813"
            },
            "WebShop": {
                "score": "0.727",
                "accuracy": "0.319",
                "grounding": "0.954"
            },
            "WebArena": {
                "score": "0.239",
                "accuracy": "0.057",
                "grounding": "0.931"
            },
            "web": {
                "score": "0.483",
                "accuracy": "0.188",
                "grounding": "0.943"
            },
            "Tool-Query": {
                "score": "0.714",
                "accuracy": "0.400",
                "grounding": "0.931"
            },
            "Tool-Operation": {
                "score": "0.405",
                "accuracy": "0.175",
                "grounding": "0.805"
            },
            "tools": {
                "score": "0.559",
                "accuracy": "0.288",
                "grounding": "0.868"
            },
            "Avg": {
                "score": "0.385",
                "accuracy": "0.172",
                "grounding": "0.718"
            }
        }
    }
]